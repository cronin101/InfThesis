\addtocontents{toc}{\protect\newpage}
\chapter{Design}

\section{System architecture}
\input{./design/architecture.tex}

\section{Design choices}
\subsection{Type annotation}
When parallelising computation using the RubiCL library, a dataset is initially `cast' to the C-type equivalent. To signify the end of a computation it is finally `cast' back to the Ruby type.

This method of redirecting a method chain using a wrapped object is intentionally similar to \verb|Enumerable#lazy| in Ruby's standard library.

\verb|Enumerable#lazy| allows computation to be deferred until is known how many results are needed. In some cases, such as the example presented in Figure~\ref{lst:example_lazy}, computation can be avoided when the results would be discarded.

\begin{lstlisting}[
  language=Ruby,
  label=lst:example_lazy,
  caption=Redirecting a computation through Enumerable\#lazy.
]
def side_effect_increment(x, str)
  puts str
  x + 1
end

(1..5).map { |x| side_effect_increment x, "Non-lazy" }
    .take_while { |x| x < 4 }
# => [2, 3]

(1..5).lazy
    .map { |x| side_effect_increment x, "Lazy" }
    .take_while { |x| x < 4 }
    .to_a
# => [2, 3]

# Non-lazy invocation evaluates 'side_effect_increment' 5 times:
# >> Non-lazy
# >> Non-lazy
# >> Non-lazy
# >> Non-lazy
# >> Non-lazy
# Lazy invocation evaluates 'side_effect_increment' 3 times:
# >> Lazy
# >> Lazy
# >> Lazy
\end{lstlisting}

Keeping the usage akin to a conceptually similar feature should make the library easier for inexperienced programmers to get to grips with.

\subsection{Eager or deferred task dispatching}
During system design, the decision had to be made whether to eagerly evaluate parallel primitives or to buffer all requests and then dispatch tasks when a result is requested. This choice is not straightforward as there are benefits to either option.

\paragraph*{Advantages of eager dispatch}
\begin{itemize}
\item The kernel build and execution stages can be pipelined. For example, this allows the code generation and compilation stages to execute on the host \ac{CPU} while the previous task is executing on the \ac{GPU}.

\item Compute device can easily be changed mid-chain. Although it will suffer a performance penalty due to the need to transfer the data buffer, having the buffer always in a consistent state allows a device well-suited in a particular primitive to pick up where another left off.

\item Simplicity. No need to study equivalence rules.
\end{itemize}

\paragraph*{Advantages of deferred pipeline}
\begin{itemize}
\item Fewer resultant \verb|Task|s to schedule. Since adjacent combinable tasks are fused, there is less work done by the \ac{OpenCL} compiler and work-group scheduler.

\item Fewer accesses to global device memory. In the setup phase of each kernel, the elements to be transformed are loaded from the global device buffer into local storage. When multiple tasks are combined into one kernel, the intermediate result remains in unsynchronised memory until the task finalizes. This causes much less stress on the compute-device's memory subsystem.

\item Fewer work-units scheduled. It is a waste of iterations to have $3$ \verb|for-loop|s each modify a collection when all operations could occur in a single loop. It is similarly wasteful to execute $N$ work-units $3$ times when only $N$ are needed.
\end{itemize}

The execution style chosen was to defer task requests and then execute optimised tasks when a result is required.

This mirrors the approach taken by many \emph{Object-Relational Mappers}, such as \verb|ActiveRecord|, combining a pipeline of queries into optimal SQL.

The assumption was made that requiring multiple passes over the input dataset was significantly more expensive than the one-off code generation tasks. This was verified by timing information collected during empirical testing.

\subsection{Construction and processing of tuple datasets}
In order to allow a greater variety of query styles to be accelerated, it became clear that tuples should be introduced as a data-type. This necessitated the development of functionality to load a dataset of tuple elements onto the compute-device. It also required supporting the execution of parallel queries, possible on standard datasets, on the new dataset type.

After careful consideration of the requirements, two methods of achieving tuple functionality appeared suitable for the project.

\paragraph*{Custom structures of data for each element} \ac{OpenCL} supports the usage of \verb|struct|s in kernel code. With this in mind, it would be possible to store each tuple continuously in memory and use member access to retrieve each slot. The size of each tuple structure can be used to guide memory management on the host. There are both advantages and disadvantages to this method of implementation. 

An advantage of using structures is the ease of supporting arbitrary length tuples. The host code only knows about the size of each tuple. This information is combined with the number of elements present when allocating the correct amount of device memory. The pre-compiled back-end code has no notion of tuple structure. Only the dynamically generated kernel code has the need to interact with slots within a tuple. As such, the structure handling code can be programmatically provided at the same point as task definition. 

One disadvantage of using structures is the need to recreate the entire dataset whenever tuples alter in length. For example, if an array of 2-element tuples is loaded to the device, and a third slot is added to each, it is necessary to initialize a new buffer and fill it by copying existing values then inserting a new slot value for each tuple. This is the same issue faced when inserting an element within a fixed-size vector. This issue would result in an increased cost of the \verb|zip| operator when creating or resizing tuples.

\paragraph*{Virtual tuples stored in disjoint datasets}
When processing tuples of length $2$, instead of creating a buffer storing tuples continuously in memory, two dataset buffers can be provided to each invoked kernel. The first buffer stores the first slot value for each tuple. Likewise, the second buffer stores all second slot values. The generated kernel code can then be designed to operate on tuples as intended, by accessing the indexed value from the buffer corresponding to the slot requested.

This method has the advantage that the creation and resizing of tuples is much less expensive. To create tuples, the second dataset, passed as a parameter to the zip operator, is simply provided to subsequent kernel invocations alongside the currently loaded buffer. When resizing tuples, the entire buffer does not need to be recreated because of a shift in individual tuple length. Instead, an extra buffer is passed to all scheduled kernels if the tuple has grown, or one buffer is released and is no longer shared if a slot has been removed.

While this method benefits from greater performance, it is more involved to implement arbitrary sized tuples compared to the previously suggested solution. The host code is now concerned with many disjoint buffers that must be managed and released after use, and all required slots must be provided as kernel arguments prior to scheduling of any execution. In short, the host code is far more coupled to the currently loaded dataset's contents than is desirable.

However, it is clear that the second solution, virtual tuples, achieves greater performance despite a more involved implementation. Therefore, it was chosen as the project's tuple implementation method. The initial implementation of tuples was limited to a $2$ slot maximum so that the design could be completed quickly and verified as correct, but it is entirely possible to expand the system to support arbitrary length tuples later.
