\chapter{Conclusions}
\section{Success of the project at achieving its aims}
\subsection{Improving the performance of dynamic languages when executing data driven tasks}
The distinct task benchmarks, shown in the \emph{Results} chapter, demonstrate that the development system achieved a speed-up of $3.5$\textendash$9$ times for all parallelised higher-order functions, given enough input data to offset task latency.
Greater improvement over standard Ruby can be obtained when users have split a complex stage of a pipeline into multiple smaller stages, for conceptual simplicity, as this does not negatively affect the project's execution performance.

In addition to excelling in artificial benchmarks, the library performed well in the realistic scenario presented during user testing, with a measured speed-up factor of $4.06$.

Importantly, increased execution throughput occurred at often-achieved dataset sizes. While the highest rates of speedup required over a million items, the RubiCL library started responding to tasks faster than the default implementation at just $20,000$\textendash$25,000$ elements for \emph{Map} and \emph{MapFilter} tasks, and $50,000$ elements when solely filtering.

However, deciding how much credit to give to parallelism for these results is another matter. Custom native-extension code was benchmarked and outperformed the library in all task types apart from pure-\emph{Map}. Furthermore, this `optimal' solution only utilised a single thread of execution, therefore it is clear that utilisation of highly-parallel devices did not boost performance past sequentially obtainable levels.

This result of parallelism being ineffective is not conclusive though, as knowing that you have produced the `optimal' parallel solution is much harder than with sequential code. It is likely that several performance breakthroughs are possible within the library's implementation, especially since high performance \ac{OpenCL} programming is still regarded as somewhat of a \emph{black art}.

It is likely that a high proportion of speed-up achieved by both the native extension and the project's solution is due to aversion of the need to un-box variables individually and then retrieve the behaviour for the operators requested, given the resultant type.

As a means to generate code that sidesteps method-cache lookup, \ac{OpenCL} works well. Its notion of \emph{kernel} dispatch is a useful metaphor when batching requested operations for execution and then retrieving results. Its inclusion into the project simplified the challenge of translating high-level function objects into low-level execution instructions, something that would be unclear how to achieve otherwise. Finally, the library provides the ability to execute on arbitrary parallel compute devices. It is possible that in future, with increases in either \ac{OpenCL} performance or \emph{processing-unit} density, a higher ratio of bespoke code performance will be obtained.
\subsection{Facilitating a larger scale of experimentation in a REPL environment by non-expert users}
User evaluation performed suggested that the complexity of the produced system was not too great for utilisation by non-experts. The library was designed to allow complete agnosticism of parallel programming techniques and paradigms for execution on non-\ac{CPU} devices. A result, it should be accessible to anyone who understands how to manipulate the results of a pipeline computation, and how higher-order functions can be used to transform datasets.

However, the sample size for usability testing was far too small too small and further research is necessary to verify ease-of-use claims with any confidence.

\emph{MapFilter} tasks, commonly used in \ac{REPL} environments when answering queries, experienced at best a $9$ times speed-up through utilisation of the project's deliverable. This result is significant when considering the usage patterns of interactive programming. With the RubiCL library, an involved query taking $10$ seconds to complete will require minimal user patience before results are returned to be interpreted. The same query could take $1.5$ minutes to complete without redirecting computation to a more efficient implementation. With this level of delay, the end-user is far more likely to become impatient and get distracted, taking a coffee break or experiencing another pitfall resultant in a costly mental context-switch.

\subsection{Exploring the extensibility of the Ruby programming language}
When designing the interface for the library, several constraints were placed on syntax in order to achieve desired functionality. Firstly, it was necessary to add annotation to the start and end of the computation pipeline. Without annotation, it would be impossible for the standard RubyVM implementation to judge whether all elements within a collection object were of a uniform type without the costly task of examining them in sequence. Secondly, without providing delimiters for the start and end of computation it would be impossible to defer tasks and dispatch fewer combined tasks later. This is because each method within a pipeline is transformed into an invocation stack-machine instruction as soon as the token corresponding to its name has terminated, either from a subsequent call or a newline token.

Given these requirements, the interface presented for specifying computation is opaque as possible. Programmers using the library have no knowledge of how work is segmented for parallel processing. The target compute-device utilised is unknown unless it has been explicitly set. The project succeeds at only inconveniencing users with providing the bare minimum of information required for its operation, and no more.

As the library's functionality is itself provided by a native extension, this project shows that expanding the capabilities of the Ruby language in this manner is a simple and effective way of speeding-up performance critical tasks. This ease of extension can be embraced by the design paradigm of first building something that works and then continually improving it. Sections of code that are producing the majority of runtime can be replaced with custom native extension or other methods of offloading tasks.

The RubiCL library allows programmers to offload computationally intensive tasks to a more efficient implementation from the start. This reduces the need to worry about revisiting these sections later, as long as the required minimum dataset sizes for speedup are reached.

\subsection{Effective code generation and reuse on the OpenCL platform}
When producing the kernel generation subsystem, it was found that the simplicity of \ac{OpenCL} syntax, much like C syntax, facilitated this task. The operation of translating parsed instruction tokens into an equivalent kernel was straightforward. However, this stage would be far more elegant if LLVM \ac{IR} could be output by the parser instead, and mapped to a compiled \ac{OpenCL} kernel object. This would avoid the fragility of string handling during code generation altogether. Furthermore, it would replace the program compilation stages of the project's back-end library, leaving it conceptually leaner as compiled kernel code can be fetched and executed instead. Capability to achieve this is currently lacking, with several projects attempting to provide LLVM-to-\ac{OpenCL} mapping but none are yet at a useful stage of development. A breakthrough here would make arbitrary task generation much easier.

Within the project's development cycle, it was found that reuse of parallel primitives was possible and this greatly reduced the duplication of coding effort. Examples of how this composition was achieved can be observed in this report's \emph{Implementation} chapter. Successful utilisation of previously defined primitives over multiple tasks was aided by the library's back-end design. At one level of abstraction, all required transformations of a dataset given a task object were defined. The native extension, used to provide method functionality to the library's Ruby runtime, merely triggered combinations of these primitives in order to perform each type of parallel task. How well this style of reuse can be achieved in other projects will vary greatly with how general the required operations over datasets are.


\subsection{Applicability to a variety of platforms, avoiding over-tailing for a specific machine}
The desktop system experienced task speed-up after utilising the project library to perform tasks. However, it did not perform as well as was expected after witnessing the initial success of the development system. Specifically, $2$\textendash$4$ times speed-up of tasks was achieved, but this pales in comparison to the factors of $3.5$\textendash$9$ displayed during laptop benchmarking.

More significantly, the number of dataset elements required for beneficial outsourcing of computation rose from tens of thousands to $2,000,000$\textendash$5,000,000$. It is likely that the greater task dispatch latency experienced on the desktop system, $300$ms as opposed to $50$ms, is to blame for the elevated lower-bound. Unfortunately, it is less clear what the cause for this increased delay is. It was expected that \ac{GPGPU} tasks would experience high latency, due to the need to transfer datasets over the \ac{PCI} bus. Yet, the delay was experienced by the \ac{CPU} device as well. After the issues faced correctly installing \ac{AMD} \ac{OpenCL} support on \emph{GNU/Linux}, it is possible that suboptimal driver performance may have some part to play. Nonetheless, this issue should be investigated further in order to increase the applicability of the project's deliverable on desktops.

The rate of \ac{GPGPU} acceleration did seem like it could reach over $4$ times that of the standard implementation when filtering the larger datasets, as it showed no sign of plateauing on the performance graphs. Without further benchmarking it will be impossible to know whether this is the case.

Another oddity experienced on the desktop system was the fact that sort task performance was lower on the \ac{GPGPU} then the \ac{CPU}. This is unexpected as the algorithm used was designed for massively parallel architectures at the expense of extra work. This hints that the hardware wasn't being utilised fully, but again only further research can determine what went wrong.

Finally, it must be reiterated that it is currently quite involved to setup a working \emph{GNU/Linux} \ac{AMD} \ac{OpenCL} compute system. The need for proprietary drivers that do not play nicely with the default graphics tool-chain to be installed before tasks can be accelerated reduces the number of eligible systems significantly.

With all the issues encountered, this goal experienced only partial success. Sizeable performance improvements were still provided for desktop system computation. However, they required a much larger dataset to offset the sizeable latency, and the system setup phase was harder than anticipated. Hopefully, installation issues will decrease over time as uptake increases.

\section{Further work}
