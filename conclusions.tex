\chapter{Conclusions}
\section{Success of the project at achieving its aims}
\subsection{Improving the performance of dynamic languages when executing data driven tasks}
The distinct task benchmarks, shown in the \emph{Results} chapter, demonstrate that the development system achieved a speed-up of $3.5$\textendash$9$ times for all parallelised higher-order functions, given enough input data to offset task latency.
Greater improvement over standard Ruby can be obtained when users have split a complex stage of a pipeline into multiple smaller stages, for conceptual simplicity, as this does not negatively affect the project's execution performance.

In addition to excelling in artificial benchmarks, the library performed well in the realistic scenario presented during user testing, with a measured speed-up factor of $4.06$.

Importantly, increased execution throughput occurred at often-achieved dataset sizes. While the highest rates of speedup required over a million items, the RubiCL library started responding to tasks faster than the default implementation at just $20,000$\textendash$25,000$ elements for \emph{Map} and \emph{MapFilter} tasks, and $50,000$ elements when solely filtering.

However, deciding how much credit to give to parallelism for these results is another matter. Custom native-extension code was benchmarked and outperformed the library in all task types apart from pure-\emph{Map}. Furthermore, this `optimal' solution only utilised a single thread of execution, therefore it is clear that utilisation of highly-parallel devices did not boost performance past sequentially obtainable levels.

This result of parallelism being ineffective is not conclusive though, as knowing that you have produced the `optimal' parallel solution is much harder than with sequential code. It is likely that several performance breakthroughs are possible within the library's implementation, especially since high performance \ac{OpenCL} programming is still regarded as somewhat of a \emph{black art}.

It is likely that a high proportion of speed-up achieved by both the native extension and the project's solution is due to aversion of the need to un-box variables individually and then retrieve the behaviour for the operators requested, given the resultant type.

As a means to generate code that sidesteps method-cache lookup, \ac{OpenCL} works well. Its notion of \emph{kernel} dispatch is a useful metaphor when batching requested operations for execution and then retrieving results. Its inclusion into the project simplified the challenge of translating high-level function objects into low-level execution instructions, something that would be unclear how to achieve otherwise. Finally, the library provides the ability to execute on arbitrary parallel compute devices. It is possible that in future, with increases in either \ac{OpenCL} performance or \emph{processing-unit} density, a higher ratio of bespoke code performance will be obtained.
\subsection{Facilitating a larger scale of experimentation in a REPL environment by non-expert users}
User evaluation performed suggested that the complexity of the produced system was not too great for utilisation by non-experts. The library was designed to allow complete agnosticism of parallel programming techniques and paradigms for execution on non-\ac{CPU} devices. A result, it should be accessible to anyone who understands how to manipulate the results of a pipeline computation, and how higher-order functions can be used to transform datasets.

However, the sample size for usability testing was far too small too small and further research is necessary to verify ease-of-use claims with any confidence.

\emph{MapFilter} tasks, commonly used in \ac{REPL} environments when answering queries, experienced at best a $9$ times speed-up through utilisation of the project's deliverable. This result is significant when considering the usage patterns of interactive programming. With the RubiCL library, an involved query taking $10$ seconds to complete will require minimal user patience before results are returned to be interpreted. The same query could take $1.5$ minutes to complete without redirecting computation to a more efficient implementation. With this level of delay, the end-user is far more likely to become impatient and get distracted, taking a coffee break or experiencing another pitfall resultant in a costly mental context-switch.

\subsection{Exploring the extensibility of the Ruby programming language}
When designing the interface for the library, several constraints were placed on syntax in order to achieve desired functionality. Firstly, it was necessary to add annotation to the start and end of the computation pipeline. Without annotation, it would be impossible for the standard RubyVM implementation to judge whether all elements within a collection object were of a uniform type without the costly task of examining them in sequence. Secondly, without providing delimiters for the start and end of computation it would be impossible to defer tasks and dispatch fewer combined tasks later. This is because each method within a pipeline is transformed into an invocation stack-machine instruction as soon as the token corresponding to its name has terminated, either from a subsequent call or a newline token.

Given these requirements, the interface presented for specifying computation is opaque as possible. Programmers using the library have no knowledge of how work is segmented for parallel processing. The target compute-device utilised is unknown unless it has been explicitly set. The project succeeds at only inconveniencing users with providing the bare minimum of information required for its operation, and no more.

As the library's functionality is itself provided by a native extension, this project shows that expanding the capabilities of the Ruby language in this manner is a simple and effective way of speeding-up performance critical tasks. This ease of extension can be embraced by the design paradigm of first building something that works and then continually improving it. Sections of code that are producing the majority of runtime can be replaced with custom native extension or other methods of offloading tasks.

The RubiCL library allows programmers to offload computationally intensive tasks to a more efficient implementation from the start. This reduces the need to worry about revisiting these sections later, as long as the required minimum dataset sizes for speedup are reached.

\subsection{Effective code generation and reuse on the OpenCL platform}
When producing the kernel generation subsystem, it was found that the simplicity of \ac{OpenCL} syntax, much like C syntax, facilitated this task. The operation of translating parsed instruction tokens into an equivalent kernel was straightforward. However, this stage would be far more elegant if LLVM \ac{IR} could be output by the parser instead, and mapped to a compiled \ac{OpenCL} kernel object. This would avoid the fragility of string handling during code generation altogether. Furthermore, it would replace the program compilation stages of the project's back-end library, leaving it conceptually leaner as compiled kernel code can be fetched and executed instead. Capability to achieve this is currently lacking, with several projects attempting to provide LLVM-to-\ac{OpenCL} mapping but none are yet at a useful stage of development. A breakthrough here would make arbitrary task generation much easier.

Within the project's development cycle, it was found that reuse of parallel primitives was possible and this greatly reduced the duplication of coding effort. Examples of how this composition was achieved can be observed in this report's \emph{Implementation} chapter. Successful utilisation of previously defined primitives over multiple tasks was aided by the library's back-end design. At one level of abstraction, all required transformations of a dataset given a task object were defined. The native extension, used to provide method functionality to the library's Ruby runtime, merely triggered combinations of these primitives in order to perform each type of parallel task. How well this style of reuse can be achieved in other projects will vary greatly with how general the required operations over datasets are.


\subsection{Applicability to a variety of platforms, avoiding over-tailing for a specific machine}
The desktop system experienced task speed-up after utilising the project library to perform tasks. However, it did not perform as well as was expected after witnessing the initial success of the development system. Specifically, $2$\textendash$4$ times speed-up of tasks was achieved, but this pales in comparison to the factors of $3.5$\textendash$9$ displayed during laptop benchmarking.

More significantly, the number of dataset elements required for beneficial outsourcing of computation rose from tens of thousands to $2,000,000$\textendash$5,000,000$. It is likely that the greater task dispatch latency experienced on the desktop system, $300$ms as opposed to $50$ms, is to blame for the elevated lower-bound. Unfortunately, it is less clear what the cause for this increased delay is. It was expected that \ac{GPU} tasks would experience high latency, due to the need to transfer datasets over the \ac{PCI} bus. Yet, the delay was experienced by the \ac{CPU} device as well. After the issues faced correctly installing \ac{AMD} \ac{OpenCL} support on \emph{GNU/Linux}, it is possible that suboptimal driver performance may have some part to play. Nonetheless, this issue should be investigated further in order to increase the applicability of the project's deliverable on desktops.

The rate of \ac{GPU} acceleration did seem like it could reach over $4$ times that of the standard implementation when filtering the larger datasets, as it showed no sign of plateauing on the performance graphs. Without further benchmarking it will be impossible to know whether this is the case.

Another oddity experienced on the desktop system was the fact that sort task performance was lower on the \ac{GPU} then the \ac{CPU}. This is unexpected as the algorithm used was designed for massively parallel architectures at the expense of extra work. This hints that the hardware wasn't being utilised fully, but again only further research can determine what went wrong.

Finally, it must be reiterated that it is currently quite involved to setup a working \emph{GNU/Linux} \ac{AMD} \ac{OpenCL} compute system. The need for proprietary drivers that do not play nicely with the default graphics tool-chain to be installed before tasks can be accelerated reduces the number of eligible systems significantly.

With all the issues encountered, this goal experienced only partial success. Sizeable performance improvements were still provided for desktop system computation. However, they required a much larger dataset to offset the sizeable latency, and the system setup phase was harder than anticipated. Hopefully, installation issues will decrease over time as uptake increases.

\section{Further work}
This section contains a mixture of work that was initially planned but left uncompleted due to time constraints, and bolder ideas for improvement that could be of interest if project development was revisited.

\paragraph*{Greater optimisation of generated kernel code} Introducing greater complexity to the \ac{OpenCL} kernel source returned by the code generation subsystem has the potential to provide further speed-up. Namely, it currently does not utilise vector instructions when performing arithmetic operations. Instead of loading a single element at a time and performing $n$ distinct additions when incrementing a dataset, $k$ elements can be retrieved from the buffer at once in each kernel invocation and processed using $k$-width vector addition. This way, only $\frac{n}{k}$ distinct operations are scheduled on the compute-unit. Implementing this feature would also involve reducing the number of work units scheduled by a factor of $k$.
This was initially a goal during project development but was abandoned in further of creating a working system first and revisiting it to optimise later. Unfortunately, since time ran out, this previous goal was never revisited.

\paragraph*{Arbitrary length tuples} Again, as a result of striving for simplicity during the project's development phase, tuple support was only added when dealing with tuples of length $2$. Instead of loading multiple distinct buffers and having the kernel provide `virtual' tuple behaviour, a single buffer should be loaded with custom structures of data. This is more involved than the current solution, but would allow general tuple behaviour without the need to keep introducing \emph{glue} code supporting additional tuple slots.

\paragraph*{Greater number of tuple-compatible primitives} Currently the system only supports creation, mapping, filtering, and combining of tuple elements. This should be expanded to support sorting of tuples. Sorting in general should be improved to accept function arguments to sort the input elements by. After the system is expanded to permit sorting of tuples, the \verb|group-by| primitive could be implemented without difficulty. Finally, by introducing arbitrary reduction of tuple sets, functionality of frameworks, such as \emph{MapReduce}, geared for processing of $key, value$ pairs could be provided on top of the RubiCL library.

\paragraph*{Investigation of latency issues on tested desktop system} As mentioned earlier, it is unknown why the desktop system's experienced latency was significantly higher than that of the development laptop. It is worthwhile investigating if there is a simple fix to this strange behavior, by spending more time studying the behavior of the library. Latency is an important issue to overcome when producing an acceleration library designed for general use, as it rules out operation on smaller datasets that occur more commonly.

\paragraph*{Integration with the Topaz VM}
\emph{Topaz}\cite{topaz} is an alternate implementation of the Ruby programming language, written using the RPython VM tool-chain. This makes it similar to the \emph{PyPy} project, responsible for a fast alternate version of the \emph{Python} language benefiting from many optimisations including \ac{JIT} compilation.

One issue experienced by the project, due to relying on the standard RubyVM, was the fact that processed array elements are initially stored in non-continuous memory. This meant that when loading integers from a dataset created in Ruby, the project's native extension had to loop over each index and request the element with that index within the array from the VM. A relevant optimisation present in Topaz is that it keeps track of the type of elements inserted into a collection. When all elements are of a single basic type, the raw values are stored, un-boxed, in continuous memory. This storage mechanism would render the process of retrieving each variable from its memory location unnecessary. Furthermore, it would be possible to pin the array, used by the Topaz VM to store the raw values, directly into device memory and then perform \emph{Map} or \emph{Filter} tasks upon it. This would greatly reduce the amount of data copied and therefore positively affect latency.

In addition, since the Topaz VM keeps track of the basic type of values within an un-boxed array, RubiCL type annotations would no longer be required. Since pipeline delimitation is still necessary, it could be reduced to calling methods with names such as \verb|#defer| and \verb|#to_a| (cast to \verb|Array|).

\paragraph*{Concurrent cooperative processing on compute devices} This was initially attempted during the final phase of the project's developmentbut abandoned due to the original design introducing too much overhead. It should be possible to perform a task pipeline by scheduling computation on dataset subsets both compute devices concurrently. For example the speed-up factors of \ac{GPU}/\ac{CPU} \emph{MapFilter} tasks on the development laptop are roughly $8$/$6$ at both $10,000,000$ and $20,000,000$ elements. Therefore, when a \emph{MapFilter} pipeline is performed, it should be possible to place $\frac{8}{14} \times 20,000,000 \approx 11,500,000$ elements on the \ac{GPU} and $\frac{6}{14} \times 20,000,000 \approx 8,500,000$ elements on the \ac{CPU} and have the two devices finish processing tasks at roughly the same time. Moreover, processing $20,000,00$ elements should only take as long as it would take for the laptop to process $11,500,000$ elements originally. The disjoint buffers can then be combined when a result is requested using the appropriate combination operator. For example, addition would be an appropriate combination if the sum of both sub-buffers had been calculated. Difficulty stems from the need to know what ratios will produce sub-problems that finish at the same time, requiring further analysis of the parallel computation cost model. In addition, necessary movement of data can introduce significant performance penalties, such as exchanging elements between separately sorted sub-buffers prior to a scan occurring. However, these pathological cases could be detected during \emph{TaskQueue} optimisation and hybrid execution avoided in these circumstances.

\paragraph*{RubyGem deployment system}
Currently, utilising the project's deliverable requires cloning the \emph{Git} repository, generating a \emph{Makefile}, and running \verb|$ make install| before the library can be included into a Ruby runtime.
\emph{RubyGems}\cite{rubygems} is a hosting service for packaged libraries written for the Ruby language. The library container, referred to as a \emph{Gem}, handles building any native extensions that may be required.
By packaging the RubiCL library as a Gem and then hosting it on RubyGems, installation would be as simple as \verb|$ gem install rubi_cl|. The library is then available to be \verb|require|d by suitable code on the system. Making it trivial to install and subsequently outsource computation with the RubiCL library is important if many others can benefit from the project's increased calculation throughput.
