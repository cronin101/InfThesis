\chapter{Evaluation}
This chapter describes the means by which the project's success will be evaluated.
Since both subjective and objective goals are stated in the \emph{Overview} chapter, this will be taken into account when constructing evaluation criteria.

The results of individual evaluation procedures will be presented in the \emph{Results} chapter, and interpreted in the corresponding \emph{Analysis} section.

\section{Recap of project aims}
\subsection{Objective goals}
\begin{description}
\item[Improving the performance of the Ruby language when executing tasks on datasets]
In order to investigate whether exploitable performance improvements are given by the completed project, a series of scenario-driven benchmarks were constructed.
The library was then utilised to perform the scenarios and compared against competing solutions: Standard Ruby code, and bespoke native extensions developed solely to achieve the evaluated task.
Further discussion of all benchmarking will be presented in the \emph{Benchmarking} subsection.
\item[Increasing the scale of \ac{REPL} experimentation possible]
This goal depends on favourable results in the previous evaluation criteria, as improved performance should lead to better \ac{REPL} response time. However, unlike the previous benchmark's contenders, there is no longer a need to consider bespoke low-level solutions. This is due to the assumption that not all code is written upfront when investigating data using a \ac{REPL}. As such, someone employed to analyse and draw conclusions from a dataset may not have the skill-set required to produce low-level code on demand.
\end{description}

\subsection{Subjective goals}
\begin{description}
\item[Gracefully extending the Ruby language runtime]
In order to judge how well the library is designed from a usability perspective, a series of user trials were conducted.
Each trial presented a participant with a task description to solve, with the use of the RubiCL library.
Notes were taken on applicant performance, specifically whether they had difficulties using the project's deliverable.
Experimental design will be discussed in further detail in the \emph{User Evaluation} section.
\item[Effective reuse of \ac{OpenCL} code]
Progress towards this goal will be summarised later  by a discussion on code reuse within the project, highlighting any successes. Since it is hard to evaluate objectively, it will be presented as a list of what was learned over the course of the project's lifetime.
\item[Suitability for deployment on unseen systems]
Portability is hard to measure. There is a near-infinite number of system configurations that could benefit from parallelism libraries. Instead of focusing on trying to install the library on as many systems as possible, installation from scratch was attempted on a typical desktop system. The results of this installation will be presented. In addition, a list of the assumptions made and requirements for the project will be produced. These requirements will then be evaluated, in the context of what was intended during the project's conception.
\end{description}

\section{Benchmarking}
\subsection{Range of tests}
The benchmarking procedure is responsible for investigating how successful the project was with regards to it's performance-oriented goals.
As such, it must be designed in a way that demonstrates the potential performance of the system, as well as being representative of realistic usage.

The decision was made to test the variety of system primitives in isolation, in addition to a combined task that produces a fused kernel.
The range of primitives shortlisted for investigation were as follows:
\begin{itemize}
  \item A basic \verb|map| task, incrementing the dataset.
  \item A `dense' \verb|filter| task returning $50\%$ of the input data.
  \item A `sparse' \verb|filter| task returning $5\%$ of the input data.
  \item A fused \verb|mapfilter| task, consisting of the previous \verb|map| task followed by the dense \verb|filter| task.
  \item A \verb|sort| task.
\end{itemize}

\subsection{Test systems}
Two systems were used for running tests, the \emph{MacBook} used primarily for development and the ordered \ac{AMD} desktop system.
With library development and regular testing occurring primarily on the laptop, the benchmarking procedure provided an opportunity to evaluate how portable the performance characteristics of the \ac{OpenCL} framework are.

\todo{Specs etc}

\subsection{Method}
\todo{Explain benchmarking method}

\section{User Evaluation}
\subsection{Subjects}
The user evaluation procedure was designed to both showcase the potential of the project, when applied to a realistic scenario, and highlight any usability issues of the library.
7 applicants were recruited for a 5 question challenge, using RubiCL to answer questions about a dataset.

The fictitious scenario presented was that of an online banking service with two, separately stored datasets, corresponding to the triggering \verb|user_id| and transfer \verb|amount| of the past $10,000,000$ transactions. Subjects had to utilise higher-order primitives to summarise activity presented in the data, under the guise of investigating a suspicious user.

When searching for potential test-subjects, care was taken to explore a variety of backgrounds and programming proficiencies.
As such, the range of applicable skills present in users evaluated ranged from high levels of parallel programming experience to barely any programming experience whatsoever.

\subsection{Method}
Before testing began, each test-subject was shown a quick demonstration of the project's capabilities, alongside a discussion about the research goal.
The aim of this quick demonstration was to hint at how the library may be applied to the later provided problems, in addition to an introduction or refresher to Ruby syntax for higher-order functions. After the demonstration concluded, a link\cite{user_test_hints} containing brief \ac{API} documentation was provided. The majority of documentation was lifted directly from the Ruby documentation source, as the library mirrors the \verb|Enumerable| closely. The few additional methods documented were RubiCL specific functionality such as the bifurcation of tuple buffers. The hint document was provided so that stuck or novice users could remind themselves of basic language functionality. Care was taken as to not reveal how to solve problems directly.

Users were provided with a skeleton file, containing a DSL for answering several given questions. Each question accepted an anonymous function response, with bound variables signifying the resources that could be used to answer the query. To respond, each applicant would replace the body of the function, originally a placeholder method, with the required query pipeline.

Subjects were unaware that the provided dataset variables were actually file handles, pointing to large collections of integer data on disk. In addition, the test system \ac{GPGPU} was assigned as the default RubiCL compute device.

Users were encouraged to save the test file each time a function body had been specified. An analysis program, listening on file-system events in the working directory, would then call the provided functions, providing test data, and report whether answers given were correct. This gave immediate feedback to subjects as to when they should move on as particular question was solved. In addition to verifying calculated answers, reports of the time taken for each task were collated.

Notes were taken during the study, followed by brief discussion with each finished applicant. Further discussion of user evaluation results and analysis will occur in the \emph{Results} chapter.
\afterpage{
  \clearpage
  \begin{landscape}
    \lstinputlisting[
      language=Ruby,
      label=lst:ue_question,
      caption=The test file full of questions given to each subject.
    ]{/Users/cronin/Dev/Ruby/HaDope/user_test_attempt_blank.rb}
  \end{landscape}
  \clearpage
  \begin{landscape}
    \lstinputlisting[
      language=Ruby,
      label=lst:ue_answer,
      caption=Sample answers to the questions given.
    ]{/Users/cronin/Dev/Ruby/HaDope/user_test_solution.rb}

  \end{landscape}
}
