\chapter{Evaluation}
This chapter describes the means by which the project's success will be evaluated.
Since both subjective and objective goals are stated in the \emph{Overview} chapter, this will be taken into account when constructing evaluation criteria.

The results of individual evaluation procedures will be presented in the \emph{Results} chapter, and interpreted in the corresponding \emph{Analysis} section.

\section{Recap of project aims}
\subsection{Objective goals}
\begin{description}
\item[Improving Ruby language performance when executing tasks on datasets]
In order to investigate whether exploitable performance improvements are given by the completed project, a series of scenario-driven benchmarks were constructed.
The library was then utilised to perform the scenarios and execution duration was compared to competing solutions: Standard Ruby code, and bespoke native extensions developed solely to achieve the evaluated task.
Further discussion of all benchmarking will be presented in the \emph{Benchmarking} subsection.
\item[Increasing the scale of \ac{REPL} experimentation possible]
This goal depends on favourable results in the previous evaluation criteria, as improved performance should lead to better \ac{REPL} response time. However, unlike the previous benchmark's contenders, there is no longer a need to consider bespoke low-level solutions. This is due to the assumption that not all code is written upfront when investigating data using a \ac{REPL}. As such, someone employed to analyse and draw conclusions from a dataset may not have the skill-set required to produce low-level code on demand.
\end{description}

\subsection{Subjective goals}
\begin{description}
\item[Gracefully extending the Ruby language runtime]
In order to judge how well the library is designed from a usability perspective, a series of user trials were conducted.
Each trial presented a participant with a task description to solve, with the use of the RubiCL library.
Notes were taken on applicant performance, specifically whether they had difficulties using the project's deliverable.
Experimental design will be discussed in further detail in the \emph{User Evaluation} section.
\item[Effective reuse of \ac{OpenCL} code]
Progress towards this goal will be summarised later  by a discussion on code reuse within the project, highlighting any successes. Since it is hard to evaluate objectively, it will be presented as a list of what was learned over the course of the project's lifetime.
\item[Suitability for deployment on unseen systems]
Portability is hard to measure. There is a near-infinite number of system configurations that could benefit from parallelism libraries. Instead of focusing on trying to install the library on as many systems as possible, installation from scratch was attempted on a typical desktop system. The results of this installation will be presented. In addition, a list of the assumptions made and requirements for the project will be produced. These requirements will then be evaluated, in the context of what was intended during the project's conception.
\end{description}

\section{Benchmarking}
\subsection{Range of tests}
The benchmarking procedure is responsible for investigating how successful the project was with regards to it's performance-oriented goals.
As such, it must be designed in a way that demonstrates the potential performance of the system, as well as being representative of realistic usage.

The decision was made to test the variety of system primitives in isolation, in addition to a combined task that produces a fused kernel.
The range of primitives shortlisted for investigation were as follows:
\begin{itemize}
  \item A basic \verb|map| task, incrementing the dataset.
  \item A `dense' \verb|filter| task returning $50\%$ of the input data.
  \item A `sparse' \verb|filter| task returning $5\%$ of the input data.
  \item A fused \verb|mapfilter| task, consisting of the previous \verb|map| task followed by the dense \verb|filter| task.
  \item A \verb|sort| task.
\end{itemize}

\subsection{Test systems}
\subsubsection{Hardware}
Two systems were used for running tests, the \emph{MacBook} laptop used primarily for development and the ordered \ac{AMD} desktop system.
With library development and regular testing occurring primarily on the laptop, the benchmarking procedure provided an opportunity to evaluate how portable the performance characteristics of the \ac{OpenCL} framework are.

\todo{Specs etc}
\subsubsection{Software}
For both hardware systems, several methods of performing each specified task were measured:
\begin{itemize}
    \item The standard implementation provided by unmodified Ruby $2.2$.
    \item A handwritten native extension that performs the task using the optimal sequential method. (Listing\ref{lst:bespoke_c})
    \item The RubiCL library performing the task, executing on the system \ac{CPU}.
    \item The RubiCL library performing the task, executing on the system \ac{GPU}.
\end{itemize}

The standard implementation was included as it is important that a comparison between any new solutions and existing functionality is made.

The reason to additionally measure the performance of a bespoke native extension performing the task is as follows:

By investigating sequential, best-case performance, we can deduce whether performance gains stem purely from parallelism or more-optimal execution. If the library exceeds performance of the best possible sequential implementation, throughput of compute devices has been harnessed most effectively. Otherwise, assuming that the standard implementation performance is exceeded, one of two things is true: Either the library provides throughput gains but the overhead of scheduling parallel tasks through \ac{OpenCL} mitigates any potential improvements. Or, the throughput provided by the increased number of execution units is not enough to compensate for the extra work encountered by the parallel algorithms employed.

The project's performance on both hardware devices present in the test system was measured, again by recording task execution duration, so that the relationship between the type of task and the optimal device architecture can be investigated.

\lstinputlisting[
  language=Ruby,
  label=lst:bespoke_c,
  caption=Custom native extension performing benchmarked tasks in the optimal sequential manner.
]{/Users/cronin/Dev/Ruby/HaDope/bespoke_extension/bespoke_backend.c}

\subsection{Variety of data-types}
The RubiCL library supports accelerating computation on homogeneous collections of both \verb|Fixnum| and \verb|Float| objects.
However, presenting benchmarks of the full range of parallel primitives on both types and both systems, then commenting in depth on both graphs, would provide a lot of redundant data.

Instead, the performance of the library on integer datasets, across both hardware systems, will be explored in depth.
This will be followed by a brief analysis of how performance differs when presented with floating-point datasets.

 The major difference when operating on \verb|Float| objects, is that they lack tagged-pointers. Therefore, the value of each object present must be determined by dereferencing the object pointer and examining the resultant \verb|RFloat| struct. This is impossible to achieve in parallel on an external hardware device, unlike when bit-shifting the tagged-pointers of \verb|Fixnum| objects. In addition,  new \verb|RFloat| objects must be created to wrap the computed results.
 These two stages of extra computation must also be performed by the RubyVM implementation, therefore the RubiCL library should not be significantly disadvantaged.

 It is also worth mentioning the difference in hardware performance concerning integer and floating-point operations. While tasks scheduled on the \ac{CPU} should utilise the same execution units as the RubyVM, tasks scheduled on the \ac{GPU} may be affected by how well the hardware is suited to floating-point calculations.

 The two factors mentioned result in a linear increase in pre-processing and post-processing time, and a possible rate-of-computation shift. Therefore, they can be summarised by merely presenting the lower-bound at which computation becomes worth outsourcing for both data-types, alongside measurement of the total speed-up rate achievable.

 \paragraph*{Lack of `double' support on Intel HD5000}
 Unfortunately, the \ac{GPU} present on the testing laptop does not support calculation on double precision floating-point numbers.
 As a result, the floating-point benchmarks presented only document \ac{CPU} performance. This means that merely the linear processing addition, and not the rate-of-computation change, can be commented on.

\subsection{Method}
\subsubsection{Gathering results}
In order to speed up the process of gathering many benchmark results, a simple utility was produced. Listing~\ref{lst:bm_helper} shows the internals of the \emph{benchmark helper} utility.

Since Ruby is a language that provides automatic memory management and garbage collection of objects, this must be disabled when benchmarking. Otherwise, the large number of repeated tests may trigger a sudden stop-the-world sweep during the latter timing rounds and skew the results.

To ensure that the results gathered represented average use effectively, each benchmark, at a particular dataset size,  was run $20$ times. Task execution duration was recorded over all repetitions, the mean of which was then returned. This lessens the weighting of anomalous readings, when the system's external utilisation may have reduced performance slightly.
\lstinputlisting[
  language=Ruby,
  label=lst:bm_helper,
  caption=Helper function defined for benchmarking a block of code.
]{/Users/cronin/Dev/Ruby/HaDope/bench_helper.rb}

Gathering readings using the helper function involved substituting the block argument with the implementation to test, and calling the utility over a range of sizes.
Once obtained, the readings for each set of competing software elements were written to disk, available for the graph generation process to consume.
Listing~\ref{lst:bm_example} shows a typical method of gathering the set of results for a standard Ruby \emph{MapFilter} task.
\lstinputlisting[
  language=Ruby,
  label=lst:bm_example,
  caption=Using the benchmark helper to gather readings over a range of input datasets.
]{/Users/cronin/Dev/Ruby/HaDope/benchmark_example.rb}

\subsection{Issues}
One issue experienced whilst collecting results was the fluctuation in \ac{APU} performance on the test laptop.

Likely due to system heat management protocols, if too many benchmarks were run in quick succession, the performance of the on-board \ac{GPU} would decrease.

This was unrealised for a significant portion of the project's implementation phase. Initial benchmarking methods, relied on throughout library construction, ran all tests at once, with the \ac{CPU} system and then the \ac{GPU} system being tested. As a result, laptop \ac{GPU} performance was consistently under-estimated. Luckily, this outside factor was realised as final benchmarks were being designed. As a result, care was taken to reduce the length of benchmark runs and instead schedule more subsets of the desired test range, allowing device temperature to stabilise between measurements.

\section{User Evaluation}
\subsection{Subjects}
The user evaluation procedure was designed to both showcase the potential of the project, when applied to a realistic scenario, and highlight any usability issues of the library.
7 applicants were recruited for a 5 question challenge, using RubiCL to answer questions about a dataset.

The fictitious scenario presented was that of an online banking service with two, separately stored datasets, corresponding to the triggering \verb|user_id| and transfer \verb|amount| of the past $10$ million transactions. Subjects had to utilise higher-order primitives to summarise activity presented in the data, under the guise of investigating a suspicious user.

When searching for potential test-subjects, care was taken to explore a variety of backgrounds and programming proficiencies.
As such, the range of applicable skills present in users evaluated ranged from high levels of parallel programming experience to barely any programming experience whatsoever.

\subsection{Method}
Before testing began, each test-subject was shown a quick demonstration of the project's capabilities, alongside a discussion about the research goal.
The aim of this quick demonstration was to hint at how the library may be applied to the later provided problems, in addition to an introduction or refresher to Ruby syntax for higher-order functions. After the demonstration concluded, a link\cite{user_test_hints} containing brief \ac{API} documentation was provided. The majority of documentation was lifted directly from the Ruby documentation source, as the library mirrors the \verb|Enumerable| \ac{API} closely. The few additional methods documented were RubiCL specific functionality such as the bifurcation of tuple buffers. The hint document was provided so that stuck or novice users could remind themselves of basic language functionality. Care was taken as to not reveal how to solve problems directly.

Users were provided with a skeleton file, containing a DSL for answering several given questions. Each question accepted an anonymous function response, with bound variables signifying the resources that could be used to answer the query. To respond, each applicant would replace the body of the function, originally a placeholder method, with the required query pipeline.

Subjects were unaware that the provided dataset variables were actually file handles, pointing to large collections of integer data on disk. In addition, the test system \ac{GPU} was assigned as the default RubiCL compute device.

Users were encouraged to save the test file each time a function body had been specified. An analysis program, listening on file-system events in the working directory, would then call the provided functions, providing test data, and report whether answers given were correct. This gave immediate feedback to subjects as to when they should move on as particular question was solved. In addition to verifying calculated answers, reports of the time taken for each task were collated.

Notes were taken during the study, followed by brief discussion with each finished applicant. Further discussion of user evaluation results and analysis will occur in the \emph{Results} chapter.

\subsubsection{Issues}
User evaluation took place late in the project's life-cycle. As a result, many potential candidates were reluctant to participate as they were busy working on their own projects.
Scheduling this session earlier would have helped avoid this.

Another reason to schedule the session earlier is the benefit of using feedback to guide design.
Luckily, most constructive criticism resultant from the user evaluation trials was simple enough to fix quickly.
However, any significant issues would be realised too late before the project's deadline to justify an intrusive overhaul.
\afterpage{
  \clearpage
  \begin{landscape}
    \lstinputlisting[
      language=Ruby,
      label=lst:ue_question,
      caption=The test file full of questions given to each subject.
    ]{/Users/cronin/Dev/Ruby/HaDope/user_test_attempt_blank.rb}
  \end{landscape}
  \clearpage
  \begin{landscape}
    \lstinputlisting[
      language=Ruby,
      label=lst:ue_answer,
      caption=Sample answers to the questions given.
    ]{/Users/cronin/Dev/Ruby/HaDope/user_test_solution.rb}

  \end{landscape}
}
